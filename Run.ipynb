{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f14334df-9eb7-4a5c-931a-0d61bad9ebb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T15:31:11.557751Z",
     "iopub.status.busy": "2025-09-21T15:31:11.557413Z",
     "iopub.status.idle": "2025-09-21T15:31:15.889317Z",
     "shell.execute_reply": "2025-09-21T15:31:15.888423Z",
     "shell.execute_reply.started": "2025-09-21T15:31:11.557718Z"
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Threat Hunting AI – AWS Hackathon \n",
    "# ================================================================\n",
    "# Run with Boss of the SOC (BOTS) Dataset Version 1 (https://github.com/splunk/botsv1)\n",
    "# or AWS permissions for CloudTrail/CloudWatch are restricted (No ingested data)\n",
    "# it falls back to synthetic/generative events so the pipeline runs\n",
    "\n",
    "import os, re, io, json, gzip, random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import IsolationForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6fffec-be0e-42e0-9484-b47d13da0462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T15:31:15.892782Z",
     "iopub.status.busy": "2025-09-21T15:31:15.891246Z",
     "iopub.status.idle": "2025-09-21T15:31:15.897095Z",
     "shell.execute_reply": "2025-09-21T15:31:15.895991Z",
     "shell.execute_reply.started": "2025-09-21T15:31:15.892749Z"
    }
   },
   "outputs": [],
   "source": [
    "#This makes the pipeline environment-aware: if AWS access is restricted, it automatically falls back to synthetic logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d3f552-f44c-4490-b1be-60c8666d1327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T15:31:15.899761Z",
     "iopub.status.busy": "2025-09-21T15:31:15.899448Z",
     "iopub.status.idle": "2025-09-21T15:31:16.203525Z",
     "shell.execute_reply": "2025-09-21T15:31:16.202293Z",
     "shell.execute_reply.started": "2025-09-21T15:31:15.899728Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## can pull CloudTrail/CloudWatch logs\n",
    "## no access in hackathon sandbox -> return false to skips AWS and use synthetic data\n",
    "\n",
    "try:\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError, EndpointConnectionError\n",
    "    AWS_AVAILABLE = True\n",
    "except Exception:\n",
    "    AWS_AVAILABLE = False\n",
    "\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "\n",
    "## Synthetic SSH/auth-like logs\n",
    "def generate_synthetic(n_normal=800, n_attacks=60) -> pd.DataFrame:\n",
    "    normals = [\n",
    "        \"Accepted password for user from 10.0.0.{i} port {p} ssh2\",\n",
    "        \"session opened for user ubuntu by (uid=1000)\",\n",
    "        \"CRON[pid]: (root) CMD (run-parts /etc/cron.hourly)\",\n",
    "        \"systemd[1]: Started Daily apt download\",\n",
    "        \"sudo: user : TTY=pts/{t} ; PWD=/home/user ; USER=root ; COMMAND=/usr/bin/apt update\",\n",
    "        \"Accepted publickey for ec2-user from 10.0.1.{i} port {p} ssh2\",\n",
    "    ]\n",
    "    attacks = [\n",
    "        \"Failed password for root from 185.213.2.{i} port {p} ssh2\",\n",
    "        \"Failed password for invalid user admin from 45.83.1.{i} port {p} ssh2\",\n",
    "        \"sudo: pam_unix(sudo:auth): authentication failure; tty=/dev/pts/{t} rhost=193.0.2.{i}\",\n",
    "        \"Suspicious lateral movement: new SSH key added for user ubuntu from 198.51.100.{i}\",\n",
    "        \"Port scan detected from 203.0.113.{i}\",\n",
    "    ]\n",
    "    rows=[]\n",
    "    for _ in range(n_normal):\n",
    "        rows.append({\"message\": random.choice(normals).format(i=random.randint(1,240), p=random.randint(1000,65000), t=random.randint(0,6))})\n",
    "    for _ in range(n_attacks):\n",
    "        rows.append({\"message\": random.choice(attacks).format(i=random.randint(1,240), p=random.randint(1000,65000), t=random.randint(0,6))})\n",
    "    random.shuffle(rows)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"source\"] = \"synthetic\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8b8d880-84a5-40c2-8bc7-6b9e26ad9eb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T15:31:16.205127Z",
     "iopub.status.busy": "2025-09-21T15:31:16.204609Z",
     "iopub.status.idle": "2025-09-21T15:31:16.210447Z",
     "shell.execute_reply": "2025-09-21T15:31:16.209585Z",
     "shell.execute_reply.started": "2025-09-21T15:31:16.205092Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "# ---- CloudTrail regions to try (skip or edit as needed) \n",
    "CLOUDTRAIL_REGIONS = (\"us-east-1\",\"us-east-2\",\"us-west-2\",\"ap-southeast-1\",\"ap-southeast-2\") \n",
    "\n",
    "# point to saved BOTSv1 files \n",
    "BOTSV1_LOCAL_GLOBS = [\n",
    "    \"/tmp/botsv1/*.csv.gz\",        \n",
    "    \"/tmp/botsv1/*.json.gz\"        \n",
    "]\n",
    "BOTSV1_S3_PREFIX =\"s3://datalakethreathunting/raw/botsv1/\"\n",
    "BOTSV1_MAX_PER_FILE = 20_000      \n",
    "\n",
    "_BOTSV1_FALLBACK_COLS = [\n",
    "    \"signature\",\"event_type\",\"message\",\"msg\",\"query\",\"uri\",\"method\",\"user\",\n",
    "    \"src\",\"src_ip\",\"srcaddr\",\"dest\",\"dest_ip\",\"dstaddr\",\n",
    "    \"src_port\",\"sport\",\"dest_port\",\"dport\",\"protocol\",\"action\",\n",
    "    \"severity\",\"status\",\"rcode\",\"host\",\"sourcetype\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de348b66-e2fd-4ba6-bff8-aadcd9ab5ea2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T15:31:16.212077Z",
     "iopub.status.busy": "2025-09-21T15:31:16.211513Z",
     "iopub.status.idle": "2025-09-21T15:31:26.892482Z",
     "shell.execute_reply": "2025-09-21T15:31:26.891234Z",
     "shell.execute_reply.started": "2025-09-21T15:31:16.212042Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BOTSv1] loaded   20000 rows from /tmp/botsv1/WinSecurity.csv.gz\n",
      "[BOTSv1] loaded   20000 rows from /tmp/botsv1/stream-dns.csv.gz\n",
      "[BOTSv1] loaded   20000 rows from /tmp/botsv1/stream-http.csv.gz\n",
      "[BOTSv1] loaded   20000 rows from /tmp/botsv1/suricata.csv.gz\n",
      "[BOTSv1] loaded   20000 rows from /tmp/botsv1/sysmon.csv.gz\n",
      "Using BOTSv1 (100000) + synthetic (860) logs\n",
      "Base dataset → BOTSv1 + synthetic, rows=100860\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08/28/2016 23:59:00 PM LogName=Security Source...</td>\n",
       "      <td>botsv1:WinSecurity.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08/28/2016 23:59:00 PM LogName=Security Source...</td>\n",
       "      <td>botsv1:WinSecurity.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08/28/2016 23:59:00 PM LogName=Security Source...</td>\n",
       "      <td>botsv1:WinSecurity.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message                  source\n",
       "0  08/28/2016 23:59:00 PM LogName=Security Source...  botsv1:WinSecurity.csv\n",
       "1  08/28/2016 23:59:00 PM LogName=Security Source...  botsv1:WinSecurity.csv\n",
       "2  08/28/2016 23:59:00 PM LogName=Security Source...  botsv1:WinSecurity.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def _assemble_message(df: pd.DataFrame) -> pd.Series:\n",
    "    if \"_raw\" in df.columns:\n",
    "        return df[\"_raw\"].astype(str)\n",
    "    keep = [c for c in _BOTSV1_FALLBACK_COLS if c in df.columns]\n",
    "    if keep:\n",
    "        return df[keep].astype(str).agg(lambda r: \" \".join(f\"{k}={v}\" for k,v in r.items()), axis=1)\n",
    "    return df.astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "def _read_any(path: str, nrows: int | None) -> pd.DataFrame:\n",
    "    p = path.lower()\n",
    "    if p.endswith(\".csv\") or p.endswith(\".csv.gz\"):\n",
    "        return pd.read_csv(path, compression=\"infer\", low_memory=False, nrows=nrows)\n",
    "    if p.endswith(\".json\") or p.endswith(\".json.gz\"):\n",
    "        return pd.read_json(path, lines=True, compression=\"infer\", nrows=nrows)\n",
    "    return pd.read_csv(path, compression=\"infer\", low_memory=False, nrows=nrows)\n",
    "\n",
    "def _list_bots_files() -> list[str]:\n",
    "    out = []\n",
    "    for g in BOTSV1_LOCAL_GLOBS:\n",
    "        out.extend(glob.glob(g))\n",
    "    if BOTSV1_S3_PREFIX:\n",
    "        try:\n",
    "            import s3fs\n",
    "            fs = s3fs.S3FileSystem()\n",
    "            for key in fs.find(BOTSV1_S3_PREFIX.rstrip(\"/\") + \"/\"):\n",
    "                if key.endswith((\".csv\", \".csv.gz\", \".json\", \".json.gz\")):\n",
    "                    out.append(\"s3://\" + key)\n",
    "        except Exception as e:\n",
    "            print(\"[BOTSv1] s3fs listing error:\", e)\n",
    "    return sorted(out)\n",
    "\n",
    "def ingest_botsv1(max_per_file: int = BOTSV1_MAX_PER_FILE) -> pd.DataFrame:\n",
    "    files = _list_bots_files()\n",
    "    if not files:\n",
    "        print(\"[BOTSv1] No files found. Set BOTSV1_LOCAL_GLOBS or BOTSV1_S3_PREFIX.\")\n",
    "        return pd.DataFrame(columns=[\"message\",\"source\"])\n",
    "    frames = []\n",
    "    for path in files:\n",
    "        try:\n",
    "            dfb = _read_any(path, nrows=max_per_file)\n",
    "            if dfb.empty:\n",
    "                continue\n",
    "            msg = _assemble_message(dfb)\n",
    "            src = os.path.basename(path).replace(\".gz\",\"\")\n",
    "            frames.append(pd.DataFrame({\"message\": msg, \"source\": f\"botsv1:{src}\"}))\n",
    "            print(f\"[BOTSv1] loaded {len(dfb):>7} rows from {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[BOTSv1] read failed: {path} -> {type(e).__name__}: {e}\")\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=[\"message\",\"source\"])\n",
    "    bots = pd.concat(frames, ignore_index=True).dropna(subset=[\"message\"])\n",
    "    bots[\"message\"] = bots[\"message\"].astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    return bots\n",
    "\n",
    "\n",
    "# ---- Try BOTSv1 ingestion ----\n",
    "bots_df = ingest_botsv1()\n",
    "\n",
    "if not bots_df.empty:\n",
    "    # BOTSv1 found → use BOTH BOTSv1 + synthetic\n",
    "    synth_df = generate_synthetic()\n",
    "    df = pd.concat([bots_df[[\"message\",\"source\"]], synth_df], ignore_index=True)\n",
    "    base_source = \"BOTSv1 + synthetic\"\n",
    "    print(f\"Using BOTSv1 ({len(bots_df)}) + synthetic ({len(synth_df)}) logs\")\n",
    "else:\n",
    "    # BOTSv1 missing → fallback to synthetic only\n",
    "    df = generate_synthetic()\n",
    "    base_source = \"synthetic\"\n",
    "    print(\"Using synthetic generator only, rows=\", len(df))\n",
    "\n",
    "print(f\"Base dataset → {base_source}, rows={len(df)}\")\n",
    "display(df.head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12234aed-fda0-4aa3-809f-9d808aa7f534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T15:31:26.894309Z",
     "iopub.status.busy": "2025-09-21T15:31:26.893801Z",
     "iopub.status.idle": "2025-09-21T15:31:30.928033Z",
     "shell.execute_reply": "2025-09-21T15:31:30.926284Z",
     "shell.execute_reply.started": "2025-09-21T15:31:26.894271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CloudTrail] us-east-1: ClientError -> AccessDeniedException\n",
      "[CloudTrail] us-east-2: ClientError -> AccessDeniedException\n",
      "[CloudTrail] us-west-2: ClientError -> AccessDeniedException\n",
      "[CloudTrail] ap-southeast-1: ClientError -> AccessDeniedException\n",
      "[CloudTrail] ap-southeast-2: ClientError -> AccessDeniedException\n",
      "[CloudWatch] loaded 185 lines from 2 group(s)\n",
      "synthetic events: (185, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[I 2025-09-21 15:19:48.622 ServerApp] Saving f...</td>\n",
       "      <td>cloudwatch:/aws/sagemaker/NotebookInstances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[I 2025-09-21 13:49:58.660 ServerApp] Starting...</td>\n",
       "      <td>cloudwatch:/aws/sagemaker/studio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[W 2025-09-21 13:50:00.042 ServerApp] No sessi...</td>\n",
       "      <td>cloudwatch:/aws/sagemaker/studio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  \\\n",
       "0  [I 2025-09-21 15:19:48.622 ServerApp] Saving f...   \n",
       "1  [I 2025-09-21 13:49:58.660 ServerApp] Starting...   \n",
       "2  [W 2025-09-21 13:50:00.042 ServerApp] No sessi...   \n",
       "\n",
       "                                        source  \n",
       "0  cloudwatch:/aws/sagemaker/NotebookInstances  \n",
       "1             cloudwatch:/aws/sagemaker/studio  \n",
       "2             cloudwatch:/aws/sagemaker/studio  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## generative logs – CloudTrail & CloudWatch \n",
    "def pull_cloudtrail_event_history(regions=CLOUDTRAIL_REGIONS, max_per_region=400):\n",
    "    if not AWS_AVAILABLE:\n",
    "        print(\"[CloudTrail] boto3 not available; skipping.\")\n",
    "        return pd.DataFrame()\n",
    "    events = []\n",
    "    for rg in regions:\n",
    "        try:\n",
    "            ct = boto3.client(\"cloudtrail\", region_name=rg)\n",
    "            token = None\n",
    "            pulled = 0\n",
    "            while True:\n",
    "                kwargs = {\"MaxResults\": 50}\n",
    "                if token: kwargs[\"NextToken\"] = token\n",
    "                resp = ct.lookup_events(**kwargs)\n",
    "                for e in resp.get(\"Events\", []):\n",
    "                    rec = json.loads(e[\"CloudTrailEvent\"])\n",
    "                    msg = f'{rec.get(\"eventTime\")} {rec.get(\"eventName\")} user={rec.get(\"userIdentity\",{}).get(\"type\")} src={rec.get(\"sourceIPAddress\")} svc={rec.get(\"eventSource\")}'\n",
    "                    events.append({\"message\": msg, \"source\": f\"cloudtrail:{rg}\"})\n",
    "                    pulled += 1\n",
    "                token = resp.get(\"NextToken\")\n",
    "                if not token or pulled >= max_per_region:\n",
    "                    break\n",
    "            print(f\"[CloudTrail] {rg}: {pulled} events\")\n",
    "        except ClientError as ce:\n",
    "            print(f\"[CloudTrail] {rg}: ClientError -> {ce.response.get('Error', {}).get('Code')}\")\n",
    "        except EndpointConnectionError:\n",
    "            print(f\"[CloudTrail] {rg}: endpoint not reachable\")\n",
    "        except Exception as ex:\n",
    "            print(f\"[CloudTrail] {rg}: {type(ex).__name__}: {ex}\")\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "def pull_cloudwatch_logs(sample_groups=2, max_lines=400):\n",
    "    if not AWS_AVAILABLE:\n",
    "        print(\"[CloudWatch] boto3 not available; skipping.\")\n",
    "        return pd.DataFrame()\n",
    "    rows=[]\n",
    "    try:\n",
    "        logs = boto3.client(\"logs\")\n",
    "        groups = logs.describe_log_groups(limit=sample_groups).get(\"logGroups\", [])\n",
    "        for g in groups[:sample_groups]:\n",
    "            name = g[\"logGroupName\"]\n",
    "            streams = logs.describe_log_streams(logGroupName=name, orderBy=\"LastEventTime\", descending=True, limit=1)\n",
    "            if not streams.get(\"logStreams\"):\n",
    "                continue\n",
    "            stream = streams[\"logStreams\"][0][\"logStreamName\"]\n",
    "            ev = logs.get_log_events(logGroupName=name, logStreamName=stream, limit=max_lines).get(\"events\", [])\n",
    "            for e in ev:\n",
    "                msg = (e.get(\"message\",\"\") or \"\").strip()\n",
    "                if msg:\n",
    "                    rows.append({\"message\": msg, \"source\": f\"cloudwatch:{name}\"})\n",
    "        print(f\"[CloudWatch] loaded {len(rows)} lines from {len(groups[:sample_groups])} group(s)\")\n",
    "    except ClientError as ce:\n",
    "        print(f\"[CloudWatch] ClientError -> {ce.response.get('Error', {}).get('Code')}\")\n",
    "    except Exception as ex:\n",
    "        print(f\"[CloudWatch] {type(ex).__name__}: {ex}\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def synthesize_cloudtrail_like(n=500):\n",
    "    svc = [\"ec2.amazonaws.com\",\"iam.amazonaws.com\",\"s3.amazonaws.com\",\"sts.amazonaws.com\"]\n",
    "    names = [\"RunInstances\",\"TerminateInstances\",\"AssumeRole\",\"PutBucketPolicy\",\"CreateUser\",\"AttachRolePolicy\",\"ConsoleLogin\",\"CreateAccessKey\"]\n",
    "    users = [\"IAMUser\",\"AssumedRole\",\"Root\"]\n",
    "    rows=[]\n",
    "    rng = np.random.default_rng(42)\n",
    "    for _ in range(n):\n",
    "        rows.append({\n",
    "            \"message\": f'2025-09-21T12:{rng.integers(0,60):02d}:33Z {rng.choice(names)} user={rng.choice(users)} src=203.0.113.{rng.integers(1,255)} svc={rng.choice(svc)}',\n",
    "            \"source\": \"cloudtrail:synthetic\"\n",
    "        })\n",
    "    print(f\"[Synthetic] generated {n} CloudTrail-like events\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Try CloudTrail → CloudWatch → fallback synthetic\n",
    "ct_df = pull_cloudtrail_event_history()\n",
    "cw_df = pull_cloudwatch_logs() if ct_df.empty else pd.DataFrame()\n",
    "synthetic_df = pd.concat([ct_df, cw_df], ignore_index=True) if not cw_df.empty else ct_df\n",
    "\n",
    "if synthetic_df.empty:\n",
    "    synthetic_df = synthesize_cloudtrail_like(600)\n",
    "\n",
    "print(\"synthetic events:\", synthetic_df.shape)\n",
    "display(synthetic_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eafb79f-935a-4aa5-a61a-24cea32085e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T15:31:30.929699Z",
     "iopub.status.busy": "2025-09-21T15:31:30.929354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset rows: 101045\n"
     ]
    }
   ],
   "source": [
    "# Merge into main df\n",
    "df = pd.concat([df, synthetic_df], ignore_index=True).dropna(subset=[\"message\"])\n",
    "print(\"Combined dataset rows:\", len(df))\n",
    "\n",
    "# %% [Vectorize → IsolationForest anomaly detection]\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    max_features=4000,\n",
    "    ngram_range=(1,2),\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\"\n",
    ")\n",
    "X = vectorizer.fit_transform(df[\"message\"])\n",
    "\n",
    "iso = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=0.04,   # ~4% anomalies (tune if needed)\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "iso.fit(X)\n",
    "pred = iso.predict(X)               # -1 = anomaly, 1 = normal\n",
    "scores = -iso.score_samples(X)      # higher = more suspicious\n",
    "\n",
    "df[\"anomaly_score\"] = scores\n",
    "df[\"prediction\"]   = np.where(pred==-1, \"anomaly\", \"normal\")\n",
    "\n",
    "print(\"Prediction counts:\\n\", df[\"prediction\"].value_counts())\n",
    "\n",
    "# %% [Visualization – anomaly scores over time + top events table]\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(df[\"anomaly_score\"].values)\n",
    "plt.title(\"Threat Hunting – Anomaly Scores Over Time\")\n",
    "plt.xlabel(\"Event index\")\n",
    "plt.ylabel(\"Suspiciousness (higher = more anomalous)\")\n",
    "plt.show()\n",
    "\n",
    "TOP_N = 15\n",
    "top = df.sort_values(\"anomaly_score\", ascending=False).head(TOP_N)[[\"anomaly_score\",\"message\",\"source\"]]\n",
    "display(top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d96250-1e9a-4c95-ab1e-dab681150f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Maps those to MITRE ATT&CK techniques\n",
    "def explain(msg: str, score: float, high_thr: float, med_thr: float):\n",
    "    reasons = []\n",
    "    if re.search(r\"Failed password|authentication failure|invalid user\", msg, re.I):\n",
    "        reasons.append(\"Multiple failed authentication attempts (possible brute force) [MITRE T1110]\")\n",
    "    if re.search(r\"port scan|scan detected\", msg, re.I):\n",
    "        reasons.append(\"Port scanning behavior [Reconnaissance]\")\n",
    "    if re.search(r\"sudo:.*authentication failure\", msg, re.I):\n",
    "        reasons.append(\"Privilege escalation attempt via sudo [Privilege Escalation]\")\n",
    "    if re.search(r\"new SSH key|authorized_keys\", msg, re.I):\n",
    "        reasons.append(\"SSH key tampering (persistence / lateral movement) [TA0003/TA0008]\")\n",
    "    if re.search(r\"\\broot\\b\", msg, re.I):\n",
    "        reasons.append(\"Activity targeting root account (high impact if compromised)\")\n",
    "    if not reasons:\n",
    "        reasons.append(\"Unusual pattern vs baseline (text anomaly)\")\n",
    "\n",
    "    severity = \"LOW\"\n",
    "    if score >= high_thr: severity = \"HIGH\"\n",
    "    elif score >= med_thr: severity = \"MEDIUM\"\n",
    "    return severity + \" | \" + \"; \".join(reasons)\n",
    "\n",
    "high_thr = np.percentile(df[\"anomaly_score\"], 97)   # top 3% = HIGH\n",
    "med_thr  = np.percentile(df[\"anomaly_score\"], 90)   # 90–97% = MEDIUM\n",
    "\n",
    "#Shows Top 50 suspicious events with explanations.\n",
    "results = df.sort_values(\"anomaly_score\", ascending=False).head(50).copy()\n",
    "results[\"explanation\"] = results.apply(lambda r: explain(r[\"message\"], r[\"anomaly_score\"], high_thr, med_thr), axis=1)\n",
    "\n",
    "print(\"=== Findings (Top 50 with explanations) ===\")\n",
    "display(results[[\"anomaly_score\",\"explanation\",\"message\",\"source\"]].head(20))\n",
    "\n",
    "escalate = results[results[\"explanation\"].str.startswith(\"HIGH\")].copy()\n",
    "print(\"\\n=== ESCALATE to analyst (HIGH only) ===\")\n",
    "display(escalate[[\"anomaly_score\",\"explanation\",\"message\",\"source\"]])\n",
    "\n",
    "results.to_csv(\"threat_hunting_findings.csv\", index=False)\n",
    "escalate.to_csv(\"threat_hunting_escalations.csv\", index=False)\n",
    "print(\"Saved CSVs: threat_hunting_findings.csv, threat_hunting_escalations.csv\")\n",
    "\n",
    "# %% Agentic to do correlation + hypotheses + next actions\n",
    "ip_re = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")\n",
    "user_re = re.compile(r\"\\bfor\\s+([A-Za-z0-9_\\-]+)\\b\")\n",
    "\n",
    "def extract_indicators(msg: str):\n",
    "    ips = ip_re.findall(msg)\n",
    "    users = user_re.findall(msg)\n",
    "    if \" root \" in f\" {msg.lower()} \" and \"root\" not in users:\n",
    "        users.append(\"root\")\n",
    "    return ips, users\n",
    "##hypothesis\n",
    "def classify_tag(msg: str):\n",
    "    tags = []\n",
    "    if re.search(r\"Failed password|authentication failure|invalid user\", msg, re.I):\n",
    "        tags.append((\"BruteForce\", \"T1110\"))\n",
    "    if re.search(r\"port scan|scan detected\", msg, re.I):\n",
    "        tags.append((\"Recon/PortScan\", \"TA0043\"))\n",
    "    if re.search(r\"sudo:.*authentication failure\", msg, re.I):\n",
    "        tags.append((\"PrivEsc-Sudo\", \"T1068/T1548\"))\n",
    "    if re.search(r\"new SSH key|authorized_keys\", msg, re.I):\n",
    "        tags.append((\"Persistence-SSHKey\", \"T1098/T1090\"))\n",
    "    if re.search(r\"\\broot\\b\", msg, re.I):\n",
    "        tags.append((\"HighValueAccount-root\", \"TA0001\"))\n",
    "    if not tags:\n",
    "        tags.append((\"Anomalous-Behavior\", \"Unknown\"))\n",
    "    return tags\n",
    "\n",
    "PLAYBOOK = {\n",
    "    \"BruteForce\": {\n",
    "        \"hypothesis\": \"Adversary attempting credential brute force against SSH.\",\n",
    "        \"next_checks\": [\n",
    "            \"Count failed logins per source IP & username (last 15m)\",\n",
    "            \"Check for successful login after bursts of failures\",\n",
    "            \"Geo-IP & reputation of source addresses\",\n",
    "            \"Verify lockout thresholds & MFA\"\n",
    "        ],\n",
    "        \"look_in\": [\"auth.log / CloudWatch\", \"IAM/SSO logs\", \"VPC Flow Logs\"]\n",
    "    },\n",
    "    \"Recon/PortScan\": {\n",
    "        \"hypothesis\": \"External host scanning ports to enumerate services.\",\n",
    "        \"next_checks\": [\n",
    "            \"Aggregate destination ports & rate per source IP\",\n",
    "            \"Check WAF/VPC Flow logs for resets/denies\",\n",
    "            \"Look for subsequent exploit attempts from same IP\"\n",
    "        ],\n",
    "        \"look_in\": [\"VPC Flow Logs\", \"WAF logs\", \"ALB/NLB access logs\"]\n",
    "    },\n",
    "    \"PrivEsc-Sudo\": {\n",
    "        \"hypothesis\": \"Privilege escalation attempt via sudo or misconfig.\",\n",
    "        \"next_checks\": [\n",
    "            \"List recent sudo failures & successes\",\n",
    "            \"Review /etc/sudoers changes & package installs\",\n",
    "            \"Correlate with new services or cron edits\"\n",
    "        ],\n",
    "        \"look_in\": [\"sudo logs\", \"OS config audit\", \"CloudTrail/SSM\"]\n",
    "    },\n",
    "    \"Persistence-SSHKey\": {\n",
    "        \"hypothesis\": \"SSH authorized_keys modified for persistence/lateral movement.\",\n",
    "        \"next_checks\": [\n",
    "            \"Diff authorized_keys across hosts/users\",\n",
    "            \"Trace SSH from suspect IP to other hosts\",\n",
    "            \"Rotate keys / force MFA revalidation\"\n",
    "        ],\n",
    "        \"look_in\": [\"/home/*/.ssh/authorized_keys\", \"SSH server logs\", \"EDR telemetry\"]\n",
    "    },\n",
    "    \"HighValueAccount-root\": {\n",
    "        \"hypothesis\": \"Activity targeting root account; high impact if compromised.\",\n",
    "        \"next_checks\": [\n",
    "            \"Confirm root SSH login disabled\",\n",
    "            \"Search for password changes or sudo to root\",\n",
    "            \"Enable alerting on any root session events\"\n",
    "        ],\n",
    "        \"look_in\": [\"auth.log\", \"Hardening baseline\", \"SIEM rules\"]\n",
    "    },\n",
    "    \"Anomalous-Behavior\": {\n",
    "        \"hypothesis\": \"Unusual pattern vs. baseline; requires triage.\",\n",
    "        \"next_checks\": [\n",
    "            \"Cluster similar messages vs. historical frequency\",\n",
    "            \"Check if host/user/IP is new or rare\"\n",
    "        ],\n",
    "        \"look_in\": [\"SIEM baseline\", \"Asset inventory\", \"Change management\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "subset = results.copy()\n",
    "subset[\"severity\"] = subset[\"explanation\"].str.extract(r\"^(HIGH|MEDIUM|LOW)\")\n",
    "subset = subset[subset[\"severity\"].isin([\"HIGH\",\"MEDIUM\"])].copy()\n",
    "\n",
    "rows = []\n",
    "#get IP address and username tfrom logs\n",
    "ip_counter, user_counter = Counter(), Counter()\n",
    "hi_thr = np.percentile(df[\"anomaly_score\"], 97)\n",
    "\n",
    "for _, r in subset.iterrows():\n",
    "    msg = r[\"message\"]\n",
    "    score = float(r[\"anomaly_score\"])\n",
    "    tags = classify_tag(msg)\n",
    "    ips, users = extract_indicators(msg)\n",
    "    for ip in ips: ip_counter[ip] += 1\n",
    "    for u in users: user_counter[u] += 1\n",
    "    for tag, mitre in tags:\n",
    "        pb = PLAYBOOK[tag]\n",
    "        rows.append({\n",
    "            \"severity\": \"HIGH\" if score >= hi_thr else \"MEDIUM\",\n",
    "            \"type\": tag,\n",
    "            \"mitre\": mitre,\n",
    "            \"anomaly_score\": round(score, 6),\n",
    "            \"ip\": \", \".join(ips) if ips else \"-\",\n",
    "            \"user\": \", \".join(users) if users else \"-\",\n",
    "            \"hypothesis\": pb[\"hypothesis\"],\n",
    "            \"next_checks\": \" | \".join(pb[\"next_checks\"]),\n",
    "            \"look_in\": \", \".join(pb[\"look_in\"]),\n",
    "            \"message\": (msg[:160] + \"...\") if len(msg) > 160 else msg\n",
    "        })\n",
    "\n",
    "#output csv file\n",
    "agent_df = pd.DataFrame(rows).sort_values([\"severity\",\"anomaly_score\"], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "print(\"=== Agentic Correlation & Hypotheses ===\")\n",
    "display(agent_df[[\"severity\",\"type\",\"mitre\",\"anomaly_score\",\"ip\",\"user\",\"hypothesis\",\"next_checks\"]].head(15))\n",
    "\n",
    "print(\"\\n=== Hot Indicators (correlated) ===\")\n",
    "hot_ips = pd.DataFrame(ip_counter.most_common(10), columns=[\"ip\",\"count\"])\n",
    "hot_users = pd.DataFrame(user_counter.most_common(10), columns=[\"user\",\"count\"])\n",
    "display(hot_ips)\n",
    "display(hot_users)\n",
    "\n",
    "agent_df[agent_df[\"severity\"]==\"HIGH\"].to_csv(\"agent_escalation_package.csv\", index=False)\n",
    "print(\"\\nSaved escalation package: agent_escalation_package.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba64b69-c8ba-4228-9221-a1b53a4034d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import re\n",
    "\n",
    "# --- Helper to safely parse ISO timestamps ---\n",
    "def parse_iso(ts):\n",
    "    try:\n",
    "        return datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# --- Timeline extraction ---\n",
    "ts = []\n",
    "iso_re = re.compile(r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\")\n",
    "for m in df[\"message\"].astype(str).tolist():\n",
    "    match = iso_re.search(m)\n",
    "    ts.append(parse_iso(match.group(0)) if match else None)\n",
    "\n",
    "base = datetime.now(timezone.utc) - timedelta(minutes=len(df)//2)\n",
    "\n",
    "df[\"ts\"] = [\n",
    "    t if t else base + timedelta(seconds=i*5)\n",
    "    for i, t in enumerate(ts)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf8baa-f16b-4551-a34c-28384136c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Severity bands\n",
    "high_thr = np.percentile(df[\"anomaly_score\"], 97)\n",
    "med_thr  = np.percentile(df[\"anomaly_score\"], 90)\n",
    "def sev(s): \n",
    "    return \"HIGH\" if s>=high_thr else (\"MEDIUM\" if s>=med_thr else \"LOW\")\n",
    "df[\"severity\"] = df[\"anomaly_score\"].apply(sev)\n",
    "\n",
    "# Plot A :severity.\n",
    "plt.figure(figsize=(11,4))\n",
    "colors = {\"HIGH\":\"crimson\",\"MEDIUM\":\"orange\",\"LOW\":\"steelblue\"}\n",
    "for sv in [\"LOW\",\"MEDIUM\",\"HIGH\"]:\n",
    "    dsub = df[df[\"severity\"]==sv]\n",
    "    plt.scatter(dsub[\"ts\"], dsub[\"anomaly_score\"], s=12, alpha=0.65, label=sv, c=colors[sv])\n",
    "plt.title(\"Threat Hunting Timeline – Suspicious Events by Severity\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Anomaly Score\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot B :BOTSv1 dataset provides historic labeled logs (2016), while synthetic data simulates recent cloud events (2025)\n",
    "plt.figure(figsize=(11,5))\n",
    "for src, color in [(\"botsv1\", \"steelblue\"), (\"synthetic\", \"crimson\")]:\n",
    "    dsub = df[df[\"source\"].str.contains(src, case=False, na=False)]\n",
    "    plt.scatter(dsub[\"ts\"], dsub[\"anomaly_score\"], s=12, alpha=0.6, label=src, c=color)\n",
    "plt.title(\"Threat Hunting Timeline – BOTSv1 vs Synthetic Events\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Anomaly Score\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd9c5b2-9197-458f-8527-aaa0c3d6c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CONFIDENCE SCORE (Model + Rules) ====\n",
    "import re\n",
    "#Rule-based detection\n",
    "def rule_signal(msg: str):\n",
    "    sig = 0.0\n",
    "    if re.search(r\"Failed password|invalid user|authentication failure\", msg, re.I): sig += 0.35\n",
    "    if re.search(r\"new SSH key|suspicious lateral movement|tampering\", msg, re.I): sig += 0.40\n",
    "    if re.search(r\"root|admin\", msg, re.I): sig += 0.25\n",
    "    return min(sig, 1.0)\n",
    "\n",
    "df[\"rule_signal\"] = df[\"message\"].astype(str).map(rule_signal)\n",
    "#Hybrid confidence score\n",
    "df[\"confidence_score\"] = 0.6*df[\"anomaly_score\"] + 0.4*df[\"rule_signal\"]\n",
    "\n",
    "def explain_event(msg):\n",
    "    if \"failed\" in msg.lower():\n",
    "        return \"Multiple failed authentication attempts\"\n",
    "    elif \"ssh key\" in msg.lower():\n",
    "        return \"SSH key tampering / lateral movement\"\n",
    "    elif \"root\" in msg.lower():\n",
    "        return \"Root account activity\"\n",
    "    else:\n",
    "        return \"General anomaly detected\"\n",
    "\n",
    "df[\"explanation\"] = df[\"message\"].apply(explain_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc04e6-1196-4730-902b-ca5b25a8d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top suspicious events\n",
    "df.sort_values(\"confidence_score\", ascending=False).head(10)[\n",
    "    [\"confidence_score\",\"anomaly_score\",\"rule_signal\",\"explanation\",\"message\"]\n",
    "]\n",
    "\n",
    "# ==== ATTACK PATH GRAPH (IP ↔ Users) ====\n",
    "import networkx as nx\n",
    "\n",
    "edges = []\n",
    "for msg in df[\"message\"].astype(str).tolist():\n",
    "    m = re.search(r\"from (\\d+\\.\\d+\\.\\d+\\.\\d+)\", msg)\n",
    "    u = re.search(r\"user (\\w+)\", msg)\n",
    "    if m and u:\n",
    "        edges.append((m.group(1), u.group(1)))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "nx.draw(G, with_labels=True, node_color=\"lightblue\", font_size=8, node_size=1200)\n",
    "plt.title(\"Attack Path Graph – IPs ↔ Users\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c88fd-6ebd-4dc5-9fb8-0d4a7cfbe91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== REAL-TIME STREAMING THREAT HUNTING SIMULATION ====\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Define event templates\n",
    "event_templates = [\n",
    "    \"Failed password for invalid user {user} from {ip} port 22 ssh2\",\n",
    "    \"Accepted password for user {user} from {ip} port 22 ssh2\",\n",
    "    \"Suspicious lateral movement: new SSH key added for {user}\",\n",
    "    \"sudo: pam_unix(sudo:auth): authentication failure; user={user} from {ip}\",\n",
    "    \"Process tampering detected: {user} attempted privilege escalation from {ip}\"\n",
    "]\n",
    "\n",
    "users = [\"admin\", \"root\", \"ubuntu\", \"dbuser\", \"testuser\"]\n",
    "ips = [f\"192.168.1.{i}\" for i in range(2,50)] + [f\"185.213.2.{i}\" for i in range(200,220)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3a999-96c8-48a0-ae6b-f2c9c3b314cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live streaming loop (10 iterations demo)\n",
    "stream_data = []\n",
    "for i in range(10):  \n",
    "    msg = random.choice(event_templates).format(user=random.choice(users), ip=random.choice(ips))\n",
    "    anomaly_score = round(random.uniform(0.2,0.9), 3)\n",
    "    sev = \"HIGH\" if anomaly_score > 0.7 else (\"MEDIUM\" if anomaly_score > 0.5 else \"LOW\")\n",
    "\n",
    "    event = {\n",
    "        \"ts\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"message\": msg,\n",
    "        \"anomaly_score\": anomaly_score,\n",
    "        \"severity\": sev\n",
    "    }\n",
    "    stream_data.append(event)\n",
    "\n",
    "    # Refresh display\n",
    "    clear_output(wait=True)\n",
    "    print(f\"=== Streaming Threat Hunting Agent (demo) – Iteration {i+1}/10 ===\\n\")\n",
    "    for e in stream_data[-5:]:  # show last 5 events\n",
    "        print(f\"[{e['ts']}] {e['severity']} | {e['message']} | score={e['anomaly_score']}\")\n",
    "\n",
    "    time.sleep(1.5)  # delay to mimic real feed\n",
    "\n",
    "# ==== Automated Response (SOAR / WAF / NACL) ====\n",
    "import re, json, time, ipaddress, pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "DRY_RUN = True                # Integrate to enable real actions\n",
    "CONF_THRESHOLD = 0.90         # Minimum confidence threshold to trigger the response, can change depends on organization needs\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# ANSI color codes\n",
    "COLORS = {\n",
    "    \"LOW\": \"\\033[92m\",      # Green\n",
    "    \"MEDIUM\": \"\\033[93m\",   # Yellow\n",
    "    \"HIGH\": \"\\033[91m\",     # Red\n",
    "    \"ENDC\": \"\\033[0m\",      # Reset\n",
    "    \"BOLD\": \"\\033[1m\"\n",
    "}\n",
    "\n",
    "def automated_response(event):\n",
    "    ip_match = re.search(r\"from (\\d+\\.\\d+\\.\\d+\\.\\d+)\", event[\"message\"])\n",
    "    ip = ip_match.group(1) if ip_match else None\n",
    "    sev = event[\"severity\"]\n",
    "\n",
    "    if event[\"anomaly_score\"] >= CONF_THRESHOLD or sev == \"HIGH\":\n",
    "        print(f\"\\n{COLORS['BOLD']}{COLORS['HIGH']}🚨 [AUTO-RESPONSE TRIGGERED]{COLORS['ENDC']}\")\n",
    "        print(f\"   Suspicious event: {event['message']}\")\n",
    "        if ip:\n",
    "            print(f\"   ➡️ Blocking IP {ip} in AWS WAF / NACL (simulated)\")\n",
    "        print(f\"   ➡️ Disabling suspicious account (simulated)\")\n",
    "        print(\"   ➡️ Escalating alert to SOC analyst via Slack/Email (simulated)\\n\")\n",
    "\n",
    "# Streaming loop with color-coded severities\n",
    "stream_data = []\n",
    "for i in range(10):  \n",
    "    msg = random.choice(event_templates).format(user=random.choice(users), ip=random.choice(ips))\n",
    "    anomaly_score = round(random.uniform(0.2,0.99), 3)\n",
    "    sev = \"HIGH\" if anomaly_score > 0.7 else (\"MEDIUM\" if anomaly_score > 0.5 else \"LOW\")\n",
    "\n",
    "    event = {\n",
    "        \"ts\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"message\": msg,\n",
    "        \"anomaly_score\": anomaly_score,\n",
    "        \"severity\": sev\n",
    "    }\n",
    "    stream_data.append(event)\n",
    "\n",
    "    # Refresh display\n",
    "    clear_output(wait=True)\n",
    "    print(f\"=== Streaming Threat Hunting Agent (demo) – Iteration {i+1}/10 ===\\n\")\n",
    "    for e in stream_data[-5:]:  # show last 5 events\n",
    "        color = COLORS[e[\"severity\"]]\n",
    "        endc = COLORS[\"ENDC\"]\n",
    "        print(f\"{color}[{e['ts']}] {e['severity']} | {e['message']} | score={e['anomaly_score']}{endc}\")\n",
    "\n",
    "    # Call fake SOAR\n",
    "    automated_response(event)\n",
    "\n",
    "    time.sleep(1.5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e192f-db66-49b8-bd8a-e2109dbb84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Extract suspicious public IPs from high-confidence alerts\n",
    "if \"confidence_score\" not in df.columns:\n",
    "    raise RuntimeError(\"Column confidence_score not found. Please run scoring cell first.\")\n",
    "\n",
    "candidates = df[df[\"confidence_score\"] >= CONF_THRESHOLD].copy()\n",
    "ip_re = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")\n",
    "\n",
    "def is_public_ip(s):\n",
    "    try:\n",
    "        return ipaddress.ip_address(s).is_global\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "block_ips = set()\n",
    "for m in candidates[\"message\"].astype(str):\n",
    "    for ip in ip_re.findall(m):\n",
    "        if is_public_ip(ip):\n",
    "            block_ips.add(ip)\n",
    "\n",
    "print(f\"[AUTO-RESPONSE] Alerts >= {CONF_THRESHOLD}: {len(candidates)}\")\n",
    "print(f\"[AUTO-RESPONSE] Candidate IPs to block: {sorted(block_ips)}\")\n",
    "if not block_ips:\n",
    "    print(\"[AUTO-RESPONSE] No public IPs found, stopping here.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d17e2c0-10aa-4abc-8813-b4702a944352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOAR API endpoint\n",
    "SOAR_WEBHOOK_URL = \"\"        \n",
    "SOAR_API_KEY     = \"\"        \n",
    "\n",
    "def soar_block_ip(ip, vendor=\"SOAR\"):\n",
    "    if DRY_RUN or not SOAR_WEBHOOK_URL:\n",
    "        print(f\"[DRY-RUN] {vendor}: Would block IP => {ip}\")\n",
    "        return {\"status\":\"dry-run\",\"ip\":ip,\"vendor\":vendor}\n",
    "    try:\n",
    "        import requests\n",
    "        payload = {\n",
    "            \"action\": \"block_ip\",\n",
    "            \"ip\": ip,\n",
    "            \"source\": \"ThreatHuntingAgent\",\n",
    "            \"confidence\": \"very_high\",\n",
    "            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
    "        }\n",
    "        headers = {\"Content-Type\":\"application/json\"}\n",
    "        if SOAR_API_KEY:\n",
    "            headers[\"Authorization\"] = f\"Bearer {SOAR_API_KEY}\"\n",
    "        r = requests.post(SOAR_WEBHOOK_URL, json=payload, headers=headers, timeout=10)\n",
    "        print(f\"[SOAR] {ip} -> {r.status_code} {r.text[:200]}\")\n",
    "        return {\"status\":r.status_code,\"ip\":ip,\"vendor\":vendor}\n",
    "    except Exception as e:\n",
    "        print(f\"[SOAR] Error: {e}\")\n",
    "        return {\"status\":\"error\",\"ip\":ip,\"vendor\":vendor,\"err\":str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031f8d5-12ea-471d-94d9-e0ef44e9113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Unified Auto-Response Pipeline ===\n",
    "#SOAR+WAF+NACL\n",
    "\n",
    "DRY_RUN = True\n",
    "\n",
    "def auto_response_pipeline(block_ips, \n",
    "                           soar=True, waf=True, nacl=False, \n",
    "                           dry_run=True):\n",
    "    \"\"\"\n",
    "    block_ips: set/list of IPs to block\n",
    "    soar, waf, nacl: choose which actions to run\n",
    "    dry_run: if True, no real AWS/SOAR changes (safe demo mode)\n",
    "    \"\"\"\n",
    "    actions_log = []\n",
    "    if not block_ips:\n",
    "        print(\"[AUTO] No IPs to process.\")\n",
    "        return actions_log\n",
    "\n",
    "    print(f\"[AUTO] Processing {len(block_ips)} suspicious IP(s): {sorted(block_ips)}\")\n",
    "\n",
    "    # --- SOAR Integration ---\n",
    "    if soar:\n",
    "        for ip in sorted(block_ips):\n",
    "            actions_log.append(soar_block_ip(ip, vendor=\"SOAR\"))\n",
    "    \n",
    "    # --- AWS WAFv2 Integration ---\n",
    "    if waf and WAF_IPSET_ID and WAF_NAME:\n",
    "        added = waf_block_ips(block_ips)\n",
    "        for a in added:\n",
    "            actions_log.append({\n",
    "                \"status\": \"ok\" if not dry_run else \"dry-run\",\n",
    "                \"ip\": a, \"vendor\": \"WAFv2\"\n",
    "            })\n",
    "    \n",
    "    # --- AWS VPC NACL Integration ---\n",
    "    if nacl:\n",
    "        for ip in sorted(block_ips):\n",
    "            actions_log.append(nacl_deny_ip(ip))\n",
    "    \n",
    "    # Save results\n",
    "    if actions_log:\n",
    "        pd.DataFrame(actions_log).to_csv(\"auto_response_actions.csv\", index=False)\n",
    "        print(\"[AUTO] Saved action log: auto_response_actions.csv\")\n",
    "    else:\n",
    "        print(\"[AUTO] No automated actions executed.\")\n",
    "\n",
    "    return actions_log\n",
    "# === Suspicious IPs (detected from pipeline) ===\n",
    "suspicious_ips = [\"185.213.2.205\", \"203.0.113.77\"]\n",
    "\n",
    "# === SOAR Webhook Demo (DRY-RUN) ===\n",
    "DRY_RUN = True\n",
    "SOAR_WEBHOOK_URL = \"\"   # keep blank for demo\n",
    "SOAR_API_KEY     = \"\"   # optional\n",
    "\n",
    "def soar_block_ip(ip, vendor=\"SOAR\"):\n",
    "    if DRY_RUN or not SOAR_WEBHOOK_URL:\n",
    "        print(f\"[DRY-RUN] {vendor}: Would block IP => {ip}\")\n",
    "        return {\"status\":\"dry-run\",\"ip\":ip,\"vendor\":vendor}\n",
    "    try:\n",
    "        import requests\n",
    "        from datetime import datetime\n",
    "        payload = {\n",
    "            \"action\": \"block_ip\",\n",
    "            \"ip\": ip,\n",
    "            \"source\": \"ThreatHuntingAgent\",\n",
    "            \"confidence\": \"very_high\",\n",
    "            \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
    "        }\n",
    "        headers = {\"Content-Type\":\"application/json\"}\n",
    "        if SOAR_API_KEY:\n",
    "            headers[\"Authorization\"] = f\"Bearer {SOAR_API_KEY}\"\n",
    "        r = requests.post(SOAR_WEBHOOK_URL, json=payload, headers=headers, timeout=10)\n",
    "        print(f\"[SOAR] {ip} -> {r.status_code} {r.text[:200]}\")\n",
    "        return {\"status\":r.status_code,\"ip\":ip,\"vendor\":vendor}\n",
    "    except Exception as e:\n",
    "        print(f\"[SOAR] Error: {e}\")\n",
    "        return {\"status\":\"error\",\"ip\":ip,\"vendor\":vendor,\"err\":str(e)}\n",
    "\n",
    "# === Run demo ===\n",
    "print(f\"[AUTO] Processing suspicious IPs: {suspicious_ips}\")\n",
    "for ip in suspicious_ips:\n",
    "    soar_block_ip(ip)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
